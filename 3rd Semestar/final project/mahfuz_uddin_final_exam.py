# -*- coding: utf-8 -*-
"""Mahfuz Uddin Final Exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WDkyCvrvo9Vn7QPY7C93wgDh0ga9qrKt
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

"""1. Data Loading"""

df = pd.read_csv("diabetes.csv")
df.head()

df.shape

# 2. Data Preprocessing

# Step 1: Handle missing values
cols_with_zero = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']
df[cols_with_zero] = df[cols_with_zero].replace(0, np.nan)

# Step 2: Imputation 
imputer = SimpleImputer(strategy='median')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Step 3: Outlier handling using IQR method
Q1 = df_imputed.quantile(0.25)
Q3 = df_imputed.quantile(0.75)
IQR = Q3 - Q1
df_clean = df_imputed[~((df_imputed < (Q1 - 1.5 * IQR)) | (df_imputed > (Q3 + 1.5 * IQR))).any(axis=1)]

# Step 4: Separate features and target
X = df_clean.drop('Outcome', axis=1)
y = df_clean['Outcome']

# Step 5: Feature scaling
scaler = StandardScaler()
X_scaled_display = scaler.fit_transform(X) 

print("Preprocessing Complete.")
print(f"Original Shape: {df.shape}")
print(f"Shape after Outlier Removal: {df_clean.shape}")

"""3. Pipeline Creation"""

# preprocessing pipeline
preprocessor = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Full ML Pipeline
model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', RandomForestClassifier(random_state=42))
])

"""### 4. Primary Model Selection

Primary Model Selected - Random Forest Classifier

Random Forest Classifier is well-suited for this dataset because it handles non-linear relationships and feature interactions effectively. It is robust to noise, reduces overfitting through ensemble learning, and performs well on medical tabular data with mixed feature importance.

5. Model Training
"""

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Train the model
model_pipeline.fit(X_train, y_train)

"""6. Cross-Validation"""

# Apply 5-fold cross-validation
cv_scores = cross_val_score(
    model_pipeline,
    X_train,
    y_train,
    cv=5,
    scoring='accuracy'
)

# Calculate mean and standard deviation
cv_mean = cv_scores.mean()
cv_std = cv_scores.std()

print(f"Cross-Validation Average Score: {cv_mean:.4f}")
print(f"Standard Deviation: {cv_std:.4f}")

"""7. Hyperparameter Tuning"""

param_grid = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [None, 10],
    'model__min_samples_split': [2, 5]
}

grid = GridSearchCV(
    model_pipeline,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid.fit(X_train, y_train)
print(grid.best_score_)
print(grid.best_params_)

"""8. Best Model Selection"""

# Select the best model from GridSearchCV
best_model = grid.best_estimator_

best_model

"""9. Model Performance Evaluation"""

# 1. Prediction
y_pred = best_model.predict(X_test)

# 2. Accuracy
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# 3. Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 4. Confusion Matrix
print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Heatmap Plot
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()